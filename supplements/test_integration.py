"""
æµ‹è¯• AttentionGate ä¸ TransUNet çš„é›†æˆ
"""

import torch
import torch.nn as nn
from TransUNet_original_freeze_ce_loss import TransUNet
from AttentionGate import AttentionGate
from config import Config

def test_attention_gate():
    """æµ‹è¯• AttentionGate æ¨¡å—"""
    print("=== æµ‹è¯• AttentionGate æ¨¡å— ===")
    
    # æµ‹è¯•å‚æ•°
    batch_size = 2
    channels_enc = 64
    channels_dec = 128
    height, width = 32, 32
    
    # åˆ›å»º AttentionGate
    ag = AttentionGate(C_enc=channels_enc, C_dec=channels_dec, C_mid=32)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x_enc = torch.randn(batch_size, channels_enc, height, width)
    x_dec = torch.randn(batch_size, channels_dec, height, width)
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = ag(x_enc, x_dec)
    
    print(f"ç¼–ç å™¨è¾“å…¥å½¢çŠ¶: {x_enc.shape}")
    print(f"è§£ç å™¨è¾“å…¥å½¢çŠ¶: {x_dec.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"AttentionGate å‚æ•°é‡: {sum(p.numel() for p in ag.parameters()):,}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert output.shape == x_enc.shape, f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {output.shape} vs {x_enc.shape}"
    print("âœ“ AttentionGate å½¢çŠ¶æµ‹è¯•é€šè¿‡")
    
    return ag

def test_transunet_with_attention():
    """æµ‹è¯•å¸¦æ³¨æ„åŠ›é—¨æ§çš„ TransUNet"""
    print("\n=== æµ‹è¯•å¸¦æ³¨æ„åŠ›é—¨æ§çš„ TransUNet ===")
    
    # åˆ›å»ºæ¨¡å‹
    model_with_ag = TransUNet(
        in_channels=Config.IN_CHANNELS,
        out_channels=Config.OUT_CHANNELS,
        base_ch=Config.BASE_CH,
        freeze_vit_layers=Config.FREEZE_VIT_LAYERS,
        use_attention_gate=True
    )
    
    model_without_ag = TransUNet(
        in_channels=Config.IN_CHANNELS,
        out_channels=Config.OUT_CHANNELS,
        base_ch=Config.BASE_CH,
        freeze_vit_layers=Config.FREEZE_VIT_LAYERS,
        use_attention_gate=False
    )
    
    print(f"æ¨¡å‹é…ç½®:")
    print(f"  - è¾“å…¥é€šé“: {Config.IN_CHANNELS}")
    print(f"  - è¾“å‡ºé€šé“: {Config.OUT_CHANNELS}")
    print(f"  - åŸºç¡€é€šé“: {Config.BASE_CH}")
    print(f"  - å†»ç»“ ViT å±‚æ•°: {Config.FREEZE_VIT_LAYERS}")
    
    # è®¡ç®—å‚æ•°é‡
    total_params_with_ag = sum(p.numel() for p in model_with_ag.parameters())
    trainable_params_with_ag = sum(p.numel() for p in model_with_ag.parameters() if p.requires_grad)
    
    total_params_without_ag = sum(p.numel() for p in model_without_ag.parameters())
    trainable_params_without_ag = sum(p.numel() for p in model_without_ag.parameters() if p.requires_grad)
    
    print(f"\nå‚æ•°é‡å¯¹æ¯”:")
    print(f"  å¸¦æ³¨æ„åŠ›é—¨æ§:")
    print(f"    - æ€»å‚æ•°é‡: {total_params_with_ag:,}")
    print(f"    - å¯è®­ç»ƒå‚æ•°é‡: {trainable_params_with_ag:,}")
    print(f"    - æ¨¡å‹å¤§å°: {total_params_with_ag * 4 / 1024 / 1024:.2f} MB")
    
    print(f"  ä¸å¸¦æ³¨æ„åŠ›é—¨æ§:")
    print(f"    - æ€»å‚æ•°é‡: {total_params_without_ag:,}")
    print(f"    - å¯è®­ç»ƒå‚æ•°é‡: {trainable_params_without_ag:,}")
    print(f"    - æ¨¡å‹å¤§å°: {total_params_without_ag * 4 / 1024 / 1024:.2f} MB")
    
    # è®¡ç®—å¢åŠ çš„å‚æ•°é‡
    param_increase = total_params_with_ag - total_params_without_ag
    param_increase_mb = param_increase * 4 / 1024 / 1024
    print(f"\næ³¨æ„åŠ›é—¨æ§å¢åŠ çš„å‚æ•°é‡: {param_increase:,} ({param_increase_mb:.2f} MB)")
    
    return model_with_ag, model_without_ag

def test_forward_pass(model, model_name, input_size=256):
    """æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­"""
    print(f"\n=== æµ‹è¯• {model_name} å‰å‘ä¼ æ’­ ===")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(1, Config.IN_CHANNELS, input_size, input_size)
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        y = model(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_output_shape = (1, Config.OUT_CHANNELS, input_size, input_size)
    assert y.shape == expected_output_shape, f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {y.shape} vs {expected_output_shape}"
    print(f"âœ“ {model_name} å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
    
    return y

def test_different_input_sizes():
    """æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸"""
    print("\n=== æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸ ===")
    
    model = TransUNet(
        in_channels=Config.IN_CHANNELS,
        out_channels=Config.OUT_CHANNELS,
        base_ch=Config.BASE_CH,
        use_attention_gate=True
    )
    
    input_sizes = [224, 256, 512]
    
    for size in input_sizes:
        try:
            x = torch.randn(1, Config.IN_CHANNELS, size, size)
            with torch.no_grad():
                y = model(x)
            print(f"è¾“å…¥ {size}x{size} â†’ è¾“å‡º {y.shape}")
        except Exception as e:
            print(f"è¾“å…¥ {size}x{size} å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯• AttentionGate ä¸ TransUNet çš„é›†æˆ...")
    
    try:
        # æµ‹è¯• AttentionGate æ¨¡å—
        ag = test_attention_gate()
        
        # æµ‹è¯•å¸¦æ³¨æ„åŠ›é—¨æ§çš„ TransUNet
        model_with_ag, model_without_ag = test_transunet_with_attention()
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_forward_pass(model_with_ag, "å¸¦æ³¨æ„åŠ›é—¨æ§çš„ TransUNet", 256)
        test_forward_pass(model_without_ag, "ä¸å¸¦æ³¨æ„åŠ›é—¨æ§çš„ TransUNet", 256)
        
        # æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸
        test_different_input_sizes()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼AttentionGate å·²æˆåŠŸé›†æˆåˆ° TransUNet ä¸­ã€‚")
        
        # ä¿å­˜æµ‹è¯•æ¨¡å‹
        torch.save(model_with_ag.state_dict(), 'test_model_with_attention_gate.pth')
        print("âœ“ æµ‹è¯•æ¨¡å‹å·²ä¿å­˜ä¸º 'test_model_with_attention_gate.pth'")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
