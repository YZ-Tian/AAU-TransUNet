"""
TransUNet with AttentionGate æ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¼˜åŒ–åçš„æ³¨æ„åŠ›é—¨æ§æ¨¡å—
"""

import torch
import torch.nn.functional as F
from TransUNet_original_freeze_ce_loss import TransUNet
from AttentionGate import AttentionGate
import matplotlib.pyplot as plt
import numpy as np

def visualize_attention_maps(model, input_tensor, save_path=None):
    """å¯è§†åŒ–æ³¨æ„åŠ›æ©ç """
    model.eval()
    
    with torch.no_grad():
        # è·å–ä¸­é—´ç‰¹å¾
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹æ¨¡å‹æ¥è·å–ä¸­é—´ç‰¹å¾ï¼Œæš‚æ—¶è·³è¿‡
        output = model(input_tensor)
    
    # åˆ›å»ºç¤ºä¾‹æ³¨æ„åŠ›æ©ç ï¼ˆéšæœºç”Ÿæˆç”¨äºæ¼”ç¤ºï¼‰
    batch_size, channels, height, width = input_tensor.shape
    attention_maps = torch.rand(batch_size, 1, height, width)
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # åŸå§‹è¾“å…¥
    input_img = input_tensor[0, 0].cpu().numpy()
    axes[0].imshow(input_img, cmap='gray')
    axes[0].set_title('Input Image')
    axes[0].axis('off')
    
    # æ³¨æ„åŠ›æ©ç 
    attention_map = attention_maps[0, 0].cpu().numpy()
    axes[1].imshow(attention_map, cmap='hot')
    axes[1].set_title('Attention Map')
    axes[1].axis('off')
    
    # åŠ æƒåçš„ç‰¹å¾
    weighted_feature = (input_img * attention_map)
    axes[2].imshow(weighted_feature, cmap='gray')
    axes[2].set_title('Weighted Feature')
    axes[2].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()

def compare_models():
    """æ¯”è¾ƒå¸¦å’Œä¸å¸¦æ³¨æ„åŠ›é—¨æ§çš„æ¨¡å‹"""
    print("=== æ¨¡å‹å¯¹æ¯”åˆ†æ ===")
    
    # åˆ›å»ºæ¨¡å‹
    model_with_ag = TransUNet(
        in_channels=1,
        out_channels=3,
        base_ch=64,
        use_attention_gate=True
    )
    
    model_without_ag = TransUNet(
        in_channels=1,
        out_channels=3,
        base_ch=64,
        use_attention_gate=False
    )
    
    # è®¡ç®—å‚æ•°é‡
    params_with_ag = sum(p.numel() for p in model_with_ag.parameters())
    params_without_ag = sum(p.numel() for p in model_without_ag.parameters())
    
    print(f"å¸¦æ³¨æ„åŠ›é—¨æ§çš„æ¨¡å‹å‚æ•°é‡: {params_with_ag:,}")
    print(f"ä¸å¸¦æ³¨æ„åŠ›é—¨æ§çš„æ¨¡å‹å‚æ•°é‡: {params_without_ag:,}")
    print(f"å¢åŠ çš„å‚æ•°é‡: {params_with_ag - params_without_ag:,}")
    print(f"å¢åŠ æ¯”ä¾‹: {((params_with_ag - params_without_ag) / params_without_ag * 100):.2f}%")
    
    return model_with_ag, model_without_ag

def test_attention_gate_standalone():
    """ç‹¬ç«‹æµ‹è¯• AttentionGate æ¨¡å—"""
    print("\n=== ç‹¬ç«‹æµ‹è¯• AttentionGate ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    enc_channels = 64
    dec_channels = 128
    height, width = 32, 32
    
    # åˆ›å»º AttentionGate
    ag = AttentionGate(
        C_enc=enc_channels,
        C_dec=dec_channels,
        C_mid=32,
        use_bn=True,
        use_residual=True
    )
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x_enc = torch.randn(batch_size, enc_channels, height, width)
    x_dec = torch.randn(batch_size, dec_channels, height, width)
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = ag(x_enc, x_dec)
    
    print(f"ç¼–ç å™¨ç‰¹å¾å½¢çŠ¶: {x_enc.shape}")
    print(f"è§£ç å™¨ç‰¹å¾å½¢çŠ¶: {x_dec.shape}")
    print(f"è¾“å‡ºç‰¹å¾å½¢çŠ¶: {output.shape}")
    print(f"AttentionGate å‚æ•°é‡: {sum(p.numel() for p in ag.parameters()):,}")
    
    # éªŒè¯è¾“å‡º
    assert output.shape == x_enc.shape, "è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"
    print("âœ“ AttentionGate æµ‹è¯•é€šè¿‡")
    
    return ag

def performance_test():
    """æ€§èƒ½æµ‹è¯•"""
    print("\n=== æ€§èƒ½æµ‹è¯• ===")
    
    # åˆ›å»ºæ¨¡å‹
    model = TransUNet(
        in_channels=1,
        out_channels=3,
        base_ch=64,
        use_attention_gate=True
    )
    
    # æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸çš„æ€§èƒ½
    input_sizes = [128, 256, 512]
    batch_size = 1
    
    for size in input_sizes:
        input_tensor = torch.randn(batch_size, 1, size, size)
        
        # é¢„çƒ­
        with torch.no_grad():
            _ = model(input_tensor)
        
        # æ€§èƒ½æµ‹è¯•
        import time
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(10):
                _ = model(input_tensor)
        
        end_time = time.time()
        avg_time = (end_time - start_time) / 10
        
        print(f"è¾“å…¥å°ºå¯¸ {size}x{size}: å¹³å‡æ¨ç†æ—¶é—´ {avg_time:.4f}s")
    
    return model

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ TransUNet with AttentionGate æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # 1. æ¨¡å‹å¯¹æ¯”
        model_with_ag, model_without_ag = compare_models()
        
        # 2. ç‹¬ç«‹æµ‹è¯• AttentionGate
        ag = test_attention_gate_standalone()
        
        # 3. æ€§èƒ½æµ‹è¯•
        model = performance_test()
        
        # 4. åˆ›å»ºç¤ºä¾‹è¾“å…¥å¹¶æµ‹è¯•
        print("\n=== ç¤ºä¾‹æ¨ç†æµ‹è¯• ===")
        input_tensor = torch.randn(1, 1, 256, 256)
        
        with torch.no_grad():
            output = model(input_tensor)
        
        print(f"è¾“å…¥å½¢çŠ¶: {input_tensor.shape}")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print("âœ“ æ¨ç†æµ‹è¯•é€šè¿‡")
        
        # 5. ä¿å­˜æ¨¡å‹
        torch.save(model.state_dict(), 'demo_model.pth')
        print("âœ“ æ¼”ç¤ºæ¨¡å‹å·²ä¿å­˜ä¸º 'demo_model.pth'")
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("\nä½¿ç”¨è¯´æ˜:")
        print("1. åˆ›å»ºæ¨¡å‹: model = TransUNet(use_attention_gate=True)")
        print("2. è®­ç»ƒæ¨¡å‹: ä½¿ç”¨åŸæœ‰çš„è®­ç»ƒè„šæœ¬")
        print("3. æ¨ç†: output = model(input_tensor)")
        print("4. æ³¨æ„åŠ›é—¨æ§ä¼šè‡ªåŠ¨åœ¨è·³è·ƒè¿æ¥ä¸­å·¥ä½œ")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
